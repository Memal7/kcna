# Kubernetes Objects
This part of the repository demonstrates the **Kubernetes Objects** which is the part of [Kubernetes Fundamentals](./03_kubernetes-fundamentals.md).

---

## Chapter outcomes
After reading this page, you should be able to:
- Describe the Kubernetes objects and understand the task of each object in detail.

---

## What are Kubernetes (k8s) objects?
- K8s objects are a collection of primitives to represent the state of the whole cluster and allow configuring (declarative or imperative) the state of the cluster.
- A k8s object is a "record of intent" - once you create the object, the k8s system will constantly work to ensure that the object exists.
- By creating an object, you are effectively telling the k8s system what you want your cluster's workload to look like; this is your cluster's desired state.
- These objects describe how your workload should be handled and how to handle container orchestration issues like scheduling, self-healing, etc.
- Some core k8s objects are:
  - **Pods** where the containerized application instance run. 
  - **Controller** keeps the whole cluster in the desired state (e.g., *Deployment*, *ReplicaSet*, etc.).
  - **Service** provides a persistent access point to the containerized application inside pods (e.g., ingress etc.).
  - **Storage** provides persistence storage solution (e.g., volume, PV, PVC, etc.).
- Objects can be described in a YAML file and then sent to the API server, which validates the objects before they are created.
- An example k8s object declaration YAML looks like this:

```yaml
apiVersion: demo-app/v1 
kind: Deployment   
metadata:  
  name: nginx-deployment  
spec:  
  selector:  
    matchLabels:  
      app: nginx  
  replicas: 3  
  template:  
    metadata:  
      labels:  
        app: nginx  
    spec:  
      containers:  
       - name: sample-nginx-container  
          image: nginx:1.20  
          ports:  
          - containerPort: 80  
```
---

## Describe the concept and task of API server.
- All communication between all k8s components is done through the API server, and this is where users would access the cluster.
- Without it, communication with the cluster is not possible!
- It is a `RESTful API` over `HTTP` or `HTTPS` using `JSON`.
- Before a request is processed by k8s, it has to go through three stages:
  - **Authentication**  
    The requester needs to present a means of identity to authenticate with the API. Commonly done with a digital certificate (X.509) or with an external identity management system. K8s users are always externally managed. Service accounts can be used to authenticate technical users.
  - **Authorization**  
    Decides *what* the requester is allowed to do. In k8s, this can be done with *Role Based Access Control (RBAC)*.
  - **Admission Control**  
    Can be used to modify or validate the request. E.g., if a user tries to use a container image from an untrustworthy registry, an admission controller could block this request. Tools like the *Open Policy Agent* can be used to manage admission control externally.

---

## Describe Kubernetes workload objects.
- **Pods**
  - Pods are the smallest deployable compute unit in k8s and describe a unit of one or more containers that share an isolation layer of `namespaces` and `cgroups`.
  - In a pod there is usually a single containerized application instance based on a container image. However, pods can contain multiple containers including, e.g., *utility containers* or *logging containers*.
  - All containers inside a pod share an IP address and can share the filesystem.
  - Pods are ephemeral - no pod is ever re-deployed. If a pod dies and re-deploys again based on a container image, no state is maintained between these deployments.
  - Pods are atomic - if a container in a pod dies, the pod is unavailable including all other containers in the pod.
- **Controller**
  - Controllers are control loops that watch the state of your cluster, then make or request changes where needed. Each controller tries to move the current cluster state closer to the desired state.
  - K8s pods have a defined lifecycle. Imagine a pod running in your cluster and then a critical failure happens on the worker node where that pod is running. This means that all pods on that worker node fail. You must create new pods to recover even if the node later becomes healthy again.
  - However, you do not need to manage each pod manually. To make sure that a defined number of pods are running and any time, you can use controller objects (e.g. *Deployment*, *ReplicaSet*, *DaemonsSet*, etc.) that make sure the right number of the right kind of pod is always running, mathing the desired state you specified.
- **ReplicaSet**
  - ReplicaSet ensures a desired number of pods are running at any time.
  - Define the number of replicas for a particular pod, and the ReplicaSet ensures that the desired number of pods is running even if one of the pods becomes unavailable or unhealthy.
  - ReplicaSet can be used to scale out applications and improve their availability. They do this by starting multiple copies of a pod.
- **Deployment**
  - Deployment manages the state of the ReplicaSet, e.g. things like which container image to run or the numbers of pods to create.
  - Deployment describes the complete application lifecycle by managing multiple ReplicaSets that get updated when the application is changed by providing a new container image.
  - A Deployment is an object that can represent an application running on your cluster. When you create the Deployment, you might set the Deployment spec to specify the number of replicas of the application to be running. The k8s system reads the Deployment spec and starts that desired number of instances of your application - updating the status to match your spec. If any of those instances should fail (a status change), the k8s system responds to the difference between spec and status by correcting - in this case, starting a replacement instance.
  - Deployments are perfect to run stateless applications in k8s.
  - **Deployment strategies**
    - `recreate:` terminate the old version and release the new one
    - `Ramped (default):` release a new version in a rolling update fashion, one after the other
    - `blue/green:` release a new version alongside the old version then switch traffic
    - `canary:` release a new version to a subset of users, then proceed to a full rollout
    - `a/b testing:` release a new version to a subset of users in a precise way (HTTP headers, cookies, weight, etc.).  

    ![Deployment and ReplicaSet relation](./00_images/deployment-replica-relation.png)
- **StatefullSet**
  - StatefulSets can be used to run stateful applications like databases on k8s.
  - Stateful applications have special requirements that do not fit the ephemeral nature of pods and containers.
  - In contrast to Deployments, StatefulSets try to retain the IP addresses of Pods and give them a stable name, persistent storage, and more graceful handling of scaling and updates.
- **DaemonSet**
  - DaemonSet objects ensure that a copy of a pod runs on all (or some) worker nodes of your cluster.
  - DaemonSets is perfect to run infrastructure-related workloads, e.g., monitoring or logging tools.
- **Job**
  - Job objects create one or more pods that execute a task and terminate afterward.
  - Job objects are perfect to run one-shot scripts like database migrations or administrative tasks.
- **CronJobs**
  - CronJobs add a time-based configuration to jobs.
  - This allows running jobs periodically, e.g., doing a backup job every night at 4 am.

### An overview of Kubernetes workload objects with a short description of every component.
![Kubernetes Workload objects](./00_images/k8s-workload-objects.png)

---

## Describe Kubernetes networking (services) objects.
- **Services and Ingress objects tasks**
  - Services are the networking abstraction for access to the services that pods provide.
  - Services can be used to expose a set of pods as a network service.
  - Suppose that you have a service running in a pod that you would like to make available to the other clients running in the cluster. Every pod gets an IP address. In every k8s cluster, all pods have to be able to reach any other pod. So, we could simply let every client know our pod IP address, and they would be able to reach us.
  - Of course, that is not a very effective method. Pods can come and go, especially pods that are managed by a deployment. In deployments, pods are considered to be identical replicas that can be scaled up and down or replaced due to crashing or application upgrades. Each time a pod is destroyed and recreated, a new IP address is allocated. We cannot expect IP addresses to be stable in this environment. Additionally, if multiple pods are providing the service, we would need to communicate all of those addresses as they come and go.
  - K8s solves this problem with a service resource. This resource combines a load-balancing configuration, a way to select pods that provide the service, and a name.
- **ClusterIP**
  - ClusterIP creates a stable, cluster-wide virtual IP (VIP) address for a service and associates it with a name. All pods providing a service are given specific labels, and the service is provided with a selector for those labels. The selector is used to select pods and group them into endpoint resources, which contain all of the addresses and ports of those resources.
  - This service type can be used as a round-robin load balancer.  
    
  ![ClusterIP diagramm](./00_images/k8s-clusterIP.png)
- **NodePort**
  - The NodePort service type extends the ClusterIP by adding simple routing rules.
  - It opens a port (default between 30000-32767) on every worker node in the cluster and maps it to the ClusterIP.
  - This service type allows routing external traffic to the cluster.
- **LoadBalancer**
  - The LoadBalancer service type extends the NodePort by deploying an external load balancer instance.
  - This will only work if you’re in an environment that has an API to configure a load balancer instance, like GCP, AWS, Azure, or even OpenStack.
- **ExternalName**
  - A special service type that has no routing whatsoever.
  - ExternalName is using the k8sinternal DNS server to create a DNS alias.
  - You can use this to create a simple alias to resolve a rather complicated hostname like `my-dbs-az1-uid6498.cloud-provider.com`
  - This is especially useful if you want to reach external resources from your k8s cluster.  

  ![ClusterIP, NodePort, LB relation](./00_images/lb-nodeport-clusterip-relation.png)

### An overview of Kubernetes Networking (Services) objects with a short description of every component.
![Kubernetes Networking objects](./00_images/k8s-networking-objects.png)

---

## Describe Kubernetes storage objects.
- **Describe the concept of persistent storage in k8s**
  - Containers are not designed with persistent storage in mind, especially when that storage spans across multiple nodes.
  - Containers already had the concept of mounting volumes, but since in k8s you are not working with containers directly, k8s made volumes part of a pod, just like containers are.
  - Volumes allow sharing of data between multiple containers within the same pod. This concept allows for great flexibility when you want to use a sidecar pattern. The second purpose they serve is preventing data loss when a pod crashes and is restarted on the same node. Pods are started in a clean state, so all data is lost unless written to a volume. So, the problem with this concept is, that the volume is dependent on the pod!
  - Here is an example of a `hostPath` volume mount that is similar to a host mount introduced by Docker:  
    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
    name: mypod
    spec:
    containers:
    - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
    volumes:
    - name: test-volume
      hostPath:  # directory location on host
        path: /data  # this field is optional
        type: Directory
    ```  

  - But a cluster environment with multiple servers requires even more flexibility. So in this case, you can use cloud block storage like Amazon EBS, Google Persistent Disks, or Azure Disk Storage.
  - To make the user experience more uniform, k8s is using the Container Storage Interface (CSI) which allows the storage vendor to write a plugin (storage driver) that can be used in k8s.
  - To use this abstraction, you have two more objects that can be used:
    - **PersistentVolumes (PV):** A pod-independent storage that is defined by the admin at the cluster level. The object configuration holds information like the type of volume, volume size, access mode, unique identifiers, and information on how to mount it.
    - **PersistentVolumeClaims (PVC):** A request for storage by a user. If the cluster has multiple persistent volumes, the user can create a PVC that will reserve a persistent volume according to the user's needs. E.g., in pod definition you can say, you want 10GB of this type of storage.
### An overview of Kubernetes storage objects with a short description of each component.
![Kubernetes Storage objects](./00_images/k8s-storage-objects.png)

---

## Describe Kubernetes configuration objects.
- **Usage and benefits of configuration objects**
  - It is considered a bad practice to incorporate the configuration directly into the container build. Any configuration change would require the entire image to be rebuilt and the entire container or pod to be redeployed. This problem gets only worse when multiple environments (e.g., dev, staging, prod) are used and images must be built for every environment.
  - In k8s, this problem is solved by decoupling the configuration from the pods with a *ConfigMap* object.
- **ConfigMaps**
  - ConfigMaps can be used to store whole configuration files or variables as key-value pairs.
  - There are two possible ways to use a ConfigMap:
    - Mount a ConfigMap as a volume in Pod
    - Map variables from a ConfigMap to environment variables of a Pod.
  - Here is an example of a ConfigMap that contains a nginx configuration:
    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: nginx-config
    data:
      nginx.conf: |
        user nginx;
        worker_processes 3;
        error_log /var/log/nginx/error.log;
        ...
      	server {
          listen 80;
          server_name _;
          location / {
            root html;
            index  index.html index.htm;
          }
        }
      ```
  - Once the ConfigMap is created you can use it in a Pod:
    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.19
        ports:
        - containerPort: 80
        volumeMounts:
        - mountPath: /etc/nginx
          name: nginx-conf
        volumes:
        - name: nginx-conf
        configMap:
          name: nginx-conf
    ```

---

## Describe Kubernetes autoscaling objects.
- **Horizontal Pod Autoscaler (HPA)**
  - Adding more pods.
  - It is the most used autoscaler in k8s and scales the number of running pods in a cluster.
  - The HPA can watch deployments or replica sets and increase the number of replicas if a certain threshold is reached.
  - Imagine your pod can use 500MiB of memory and you configured a threshold of 80%. If the usage is over 400MiB (80%), a second pod will get scheduled. Now you have a capacity of 1000MiB. If 800MiB is used, a third pod will get scheduled, and so on.
  - HPA scaling calculations can also use custom or external metrics. Custom metrics target a marker of pod usage other than CPU usages, such as network traffic, memory, or a value relating to the pod’s application. External metrics measure values that do not correlate to a pod. E.g., an external metric could track the number of pending tasks in a queue.
- **Cluster Autoscaler**
  - The cluster autoscaler can add new worker nodes to the cluster if the demand increases. It works great in tandem with HPA.
  - Dynamically scaling the number of worker nodes to match current cluster utilization can help manage the costs of running k8s clusters on a cloud provider platform, especially with workloads that are designed to scale to meet current demand.
  - The cluster autoscaler checks the cluster for pods that cannot be scheduled on any existing nodes because of inadequate CPU or memory resources or because the pod’s node affinity rules or taint tolerations do not match an existing node.
- **Vertical Pod Autoscaler (VPA)**
  - Adding more CPU and memory to existing VMs.
  - VPA allows pods to increase and decrease the resource requests and limits dynamically. Vertical scaling is limited by the worker node capacity.
- **Kubernetes-based Event Driven Autoscaler (KEDA)**
  - KEDA can be used to scale the k8s workload based on events triggered by external systems.
  - Similar to the HPA, KEDA can scale deployments, replica sets, pods, etc., but also other objects such as k8s jobs.
  - With a large selection of out-of-the-box scalers, KEDA can scale to special triggers such as a database query or even the number of pods in a k8s cluster.
  - For more details look at the [Keda Concept](https://keda.sh/docs/2.8/concepts/)

---

[Next Part ▶ Cloud Native Application Delivery](./04_cloud-native-app-delivery.md)
